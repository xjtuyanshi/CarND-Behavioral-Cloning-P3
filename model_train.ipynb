{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### two or three laps of center lane driving\n",
    "##### one lap of recovery driving from the sides\n",
    "##### one lap focusing on driving smoothly around curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "import sklearn\n",
    "import cv2\n",
    "import skimage.color\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Lambda,Cropping2D,Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.client import device_lib\n",
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Batch image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "#read udacity\n",
    "with open(os.path.join('data','driving_log.csv')) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip header row.\n",
    "    next(reader)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "# read my training data\n",
    "with open(os.path.join('training_data','driving_log.csv')) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip header row.\n",
    "    next(reader)\n",
    "    for line in reader:\n",
    "        samples.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    if (path.__contains__('training_data')):\n",
    "        filename = path.split('\\\\')[-1] \n",
    "        file_path = os.path.join('training_data', 'IMG', filename)\n",
    "    else:\n",
    "        filename = path.split(\"/\")[-1]\n",
    "        file_path = os.path.join('data', 'IMG', filename)\n",
    "    image =  mpimg.imread(os.path.join(file_path))\n",
    "    return image\n",
    "\n",
    "\n",
    "#Data augmentation\n",
    "def decrease_brightness(img):\n",
    "    img_hsv = cv2.cvtColor(img,cv2.COLOR_RGB2HSV)\n",
    "    rand_num = random.uniform(0.6,0.9)\n",
    "    img_hsv[:,:,2] = rand_num *img_hsv[:,:,2]\n",
    "    return cv2.cvtColor(img_hsv,cv2.COLOR_HSV2RGB)\n",
    "\n",
    "def filp_image_angle(img,angle):\n",
    " \n",
    "    flipped_img = cv2.flip(img,1)\n",
    "    filpped_angle = angle*(-1.0)\n",
    "    return flipped_img, filpped_angle\n",
    "\n",
    "\n",
    "\n",
    "def get_processed_image_angle(img_path,angle_in_csv):\n",
    "    image = read_img(img_path)             \n",
    "    angle = float(angle_in_csv)\n",
    "      # random decrease brightness or flip\n",
    "    img_random_bright = random_decrease_brightness(image)\n",
    "    img_random_flip,angle = random_filp(img_random_bright,angle)\n",
    "    processed_img = cv2.cvtColor(img_random_flip, cv2.COLOR_RGB2YUV)\n",
    "    return processed_img,angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD ALL DATA IN ONE TIME (NOT USING GENARATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(samples,training_data=True):\n",
    "    images = []\n",
    "    angles = []\n",
    "    center_image_keep_rate = 0.8\n",
    "    correction = 0.23\n",
    "    \n",
    "    for sample in samples:\n",
    "        center_image = read_img(sample[0])\n",
    "        center_angle = float(sample[3])\n",
    "        if np.random.random < center_image_keep_rate:\n",
    "            \n",
    "            images.append(center_image)\n",
    "            angles.append(center_angle)\n",
    "            # if this steel wheel turns add right left camerea images\n",
    "        if center_angle != 0:\n",
    "            left_image = read_img(sample[1])\n",
    "            left_angle = center_angle + correction\n",
    "            right_image = read_img(sample[2])\n",
    "            right_angle = center_angle - correction\n",
    "            images.append(left_image)\n",
    "            images.append(right_image)\n",
    "            angles.append(left_angle)\n",
    "            angles.append(right_angle)\n",
    "    return images, angles\n",
    "\n",
    "def random_augment(images,angles):\n",
    "    random_decrease_brightness_rate = 0.3\n",
    "    augmented_images = images.copy()\n",
    "    augmented_angles = angles.copy()\n",
    "    \n",
    "    for image, angle in zip (images,angles):\n",
    "        \n",
    "        if angle !=0:\n",
    "            flipped_image,flipped_angle = filp_image_angle(image,angle)\n",
    "            augmented_images.append(flipped_image)\n",
    "            augmented_angles.append(flipped_angle)\n",
    "        #random_decrease_brightness\n",
    "        if np.random.random <random_decrease_brightness_rate:\n",
    "            dark_image = decrease_brightness(image)\n",
    "            dark_image_angle = angle\n",
    "            augmented_images.append(dark_image)\n",
    "            augmented_angles.append(dark_image_angle)\n",
    "    X = np.array(augmented_images)\n",
    "    y = np.array(augmented_angles)\n",
    "    return shuffle(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16433\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "raw_images,raw_angles = load_data(samples)\n",
    "\n",
    "X_train,y_train =random_augment(raw_images,raw_angles)\n",
    "\n",
    "t1=time.time()\n",
    "print(\"total seconds for loading and agumenting: {} sec\".format(round(t1-t0)))\n",
    "print('augmented images size:' +X_train.shape[0] +\"augmented angle size:\" + y_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENARATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f561ccf6ee16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mtrain_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_valid_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_samples' is not defined"
     ]
    }
   ],
   "source": [
    "def generator(samples,use_other_cameras=False, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images = []\n",
    "            angles = []\n",
    "           \n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                # get center, left,right images\n",
    "                center_image,center_angle = get_processed_image_angle(batch_sample[0],batch_sample[3])             \n",
    "    \n",
    "                if use_other_cameras:\n",
    "                    correction = 0.25 \n",
    "                    left_image,left_angle = get_processed_image_angle(batch_sample[1],center_angle + correction)   \n",
    "                    right_image,right_angle = get_processed_image_angle(batch_sample[2],center_angle - correction)   \n",
    "                    images.append(left_image)\n",
    "                    images.append(right_image)\n",
    "                    angles.append(left_angle)\n",
    "                    angles.append(right_angle)\n",
    "            images.append(center_image)\n",
    "            angles.append(center_angle)\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield shuffle(X_train, y_train)\n",
    "\n",
    "train_generator = generator(train_samples, batch_size=BATCH_SIZE)\n",
    "validation_generator = generator(validation_samples,is_valid_data=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_bins=50\n",
    "n, bins, patches = plt.hist(y_train, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resize_img(image):\n",
    "    import tensorflow as tf\n",
    "    resized = tf.image.resize_images(image, (66, 200))\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d (Cropping2D)      (None, 80, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 66, 200, 3)        0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 66, 200, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 31, 98, 24)        1824      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31, 98, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 47, 36)        21636     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 47, 36)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 22, 48)         43248     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 22, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 20, 64)         27712     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 18, 64)         36928     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 18, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 252,219\n",
      "Trainable params: 252,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 64\n",
    "BATCH_SIZE = 1024\n",
    "learning_rate = 0.0001\n",
    "##NVIDIA MODEL\n",
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation\n",
    "model.add(Cropping2D(cropping=((45,25),(0,0)),input_shape=(160,320,3)))\n",
    "model.add(Lambda(resize_img))\n",
    "model.add(Lambda(lambda x:x/255-0.5))\n",
    "model.add(Conv2D(24,(5,5),strides=(2,2),activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Conv2D(36,(5,5),strides=(2,2),activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Conv2D(48,(5,5),strides=(2,2),activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "checkpoint = ModelCheckpoint('model-{val_loss:03f}.h5',\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='auto')\n",
    "model.compile(loss='mse', optimizer='Adam')\n",
    "history_object = model.fit(X_train, y_train,validation_split =0.2,batch_size= BATCH_SIZE,shuffle=True,epochs=N_EPOCHS,callbacks=[checkpoint],verbose = 1)\n",
    "model_name = 'model_NVIDIA_'+datetime.datetime.now().strftime(\"%Y%m%d%H%M\")+'.h5'\n",
    "model.save(model_name)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 75, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 106, 6)        168       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 53, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 17, 6)          330       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 8, 6)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 8, 6)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               11640     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 85        \n",
      "=================================================================\n",
      "Total params: 22,387\n",
      "Trainable params: 22,387\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 11396 samples, validate on 2850 samples\n",
      "Epoch 1/25\n",
      " 4192/11396 [==========>...................] - ETA: 9s - loss: 0.0346"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b3d16a9d3a87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                  mode='auto')\n\u001b[0;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mhistory_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'model_WITH_MY_DATA_BASE_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d%H%M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# normalize and mean centered\n",
    "model.add(Lambda(lambda x:(x/255.0)-0.5, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((60,25),(0,0))))\n",
    "model.add(Conv2D(6,3,3,activation=\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(6,3,3,activation=\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Dense(84))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint('model-{val_loss:03f}.h5',\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='auto')\n",
    "model.compile(loss='mse', optimizer='Adam')\n",
    "history_object = model.fit(X_train, y_train,validation_split =0.2,shuffle=True,epochs=N_EPOCHS,callbacks=[checkpoint],verbose = 1)\n",
    "model_name = 'model_WITH_MY_DATA_BASE_'+datetime.datetime.now().strftime(\"%Y%m%d%H%M\")+'.h5'\n",
    "model.save(model_name)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_object_genarator = model.fit_generator(train_generator, steps_per_epoch= STEPS_PER_EPOCH,validation_data=validation_generator,validation_steps=VALIDATION_STEPS, epochs=N_EPOCHS, callbacks=[checkpoint],verbose = 1)\n",
    "model_name = 'model_fit_genarator'+datetime.datetime.now().strftime(\"%Y%m%d%H%M\")+'.h5'\n",
    "model.save(model_name)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('.\\data\\driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip header row.\n",
    "    next(reader)\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "images = []\n",
    "measurements = []\n",
    "for line in lines:\n",
    "    # use all three cameras\n",
    "    for i in range(1):\n",
    "        source_path = line[i]\n",
    "        filename = source_path.split(\"/\")[-1]\n",
    "        current_path =os.path.join('data','IMG',filename)\n",
    "        image = mpimg.imread(current_path)\n",
    "        #imgae =skimage.color.rgb2gray(image)\n",
    "        images.append(image)\n",
    "        measurement =float(line[3])\n",
    "        measurements.append(measurement)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip image\n",
    "augmented_images, augmented_measurements =[],[]\n",
    "for image, measurement in zip(images,measurements):\n",
    "    augmented_images.append(image)\n",
    "    augmented_measurements.append(measurement)\n",
    "    augmented_images.append(cv2.flip(image,1))\n",
    "    augmented_measurements.append(measurement*-1.0)\n",
    "\n",
    "X_train = np.array(augmented_images)\n",
    "y_train = np.array(augmented_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(images)\n",
    "y_train = np.array(measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper \n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# normalize and mean centered\n",
    "model.add(Lambda(lambda x:(x/255.0)-0.5, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((70,25),(0,0)))\n",
    "model.add(Conv2D(6,5,5,activation=\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(6,5,5,activation=\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Dense(84))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer ='adam')\n",
    "model.fit(X_train,y_train,validation_split =0.2,shuffle=True,epochs=8 )\n",
    "model.save('model.h5')\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch =\n",
    "    len(train_samples), validation_data = \n",
    "    validation_generator,\n",
    "    nb_val_samples = len(validation_samples), \n",
    "    nb_epoch=5, verbose=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
